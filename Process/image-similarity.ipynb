{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_INDEX_PATH = \"../data/faiss_index.bin\"\n",
    "FAISS_METADATA_PATH = \"../data/faiss_metadata.pkl\"\n",
    "DIMENSIONS = 128  # Descriptor dimensions (e.g., SIFT: 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Could not fetch image. Status code: {response.status_code}\")\n",
    "        return\n",
    "    return cv2.imdecode(np.frombuffer(response.content,dtype=np.int8),cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index():\n",
    "    try:\n",
    "        # Check if the file exists and is not empty\n",
    "        if os.path.exists(FAISS_INDEX_PATH) and os.path.getsize(FAISS_INDEX_PATH) > 0:\n",
    "            index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "            with open(FAISS_METADATA_PATH, \"rb\") as f:\n",
    "                metadata = pickle.load(f)\n",
    "            product_ids = metadata[\"product_ids\"]\n",
    "            return index, product_ids\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Index file is empty or doesn't exist.\")\n",
    "    except FileNotFoundError:\n",
    "        # Create a new index with L2 distance metric if the file is missing or empty\n",
    "        index = faiss.IndexFlatL2(DIMENSIONS)\n",
    "        return index, []\n",
    "\n",
    "# Save Faiss index and metadata\n",
    "def save_faiss_index(index, product_ids):\n",
    "    faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "    metadata = {\"product_ids\": product_ids}\n",
    "    with open(FAISS_METADATA_PATH, \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(\"Faiss index and metadata saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE HISTOGRAM DESCRIPTOR FOR IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptors(image, detector):\n",
    "    keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "    return descriptors\n",
    "\n",
    "def build_visual_vocabulary_faiss(descriptors_list, num_words):\n",
    "    all_descriptors = np.vstack(descriptors_list).astype('float32')  # Combine all descriptors\n",
    "    dimension = all_descriptors.shape[1]\n",
    "    \n",
    "    kmeans = faiss.Kmeans(d=dimension, k=num_words, niter=20, verbose=True, gpu=False)\n",
    "    kmeans.train(all_descriptors)\n",
    "    return kmeans\n",
    "\n",
    "def compute_histogram_faiss(descriptors, kmeans):\n",
    "    _, words = kmeans.index.search(descriptors.astype('float32'), 1)  # Assign descriptors to visual words\n",
    "    histogram, _ = np.histogram(words, bins=np.arange(kmeans.k + 1), range=(0, kmeans.k))\n",
    "    return histogram / np.linalg.norm(histogram)  # Normalize histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATE FIASS MODEL WITH NEW INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_descriptor_to_faiss(new_images):\n",
    "    index, product_ids = load_faiss_index()\n",
    "    sift = cv2.SIFT_create()\n",
    "    kmeans = build_visual_vocabulary_faiss(descriptors_list, num_words)\n",
    "    # Extract descriptors from new images\n",
    "    descriptors_list = []\n",
    "    for product_id, image_path in new_images:\n",
    "        image = get_image_from_url(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error loading image: {image_path}\")\n",
    "            continue\n",
    "        histo_descriptor = compute_histogram_faiss(sift, kmeans)\n",
    "        if histo_descriptor is not None:\n",
    "            descriptors_list.append(histo_descriptor)\n",
    "            product_ids.extend([product_id])\n",
    "        else:\n",
    "            print(f\"No descriptors found for image: {image_path}\")\n",
    "\n",
    "    if descriptors_list:\n",
    "        # Combine descriptors into a single matrix\n",
    "        combined_descriptors = np.vstack(descriptors_list).astype(np.float32)\n",
    "\n",
    "        # Add descriptors to Faiss index\n",
    "        index.add(combined_descriptors)\n",
    "\n",
    "        # Save updated index and metadata\n",
    "        save_faiss_index(index, product_ids)\n",
    "        print(f\"Added {len(descriptors_list)} images to Faiss index.\")\n",
    "    else:\n",
    "        print(\"No valid descriptors to add.\")\n",
    "\n",
    "# Search for similar images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_images(url, k=5):\n",
    "    index, product_ids = load_faiss_index()\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    # Extract descriptors from the query image\n",
    "    query_image = get_image_from_url(url)\n",
    "    _, query_descriptors = sift.detectAndCompute(query_image, None)\n",
    "\n",
    "    if query_descriptors is not None:\n",
    "        # Search the index\n",
    "        query_descriptors = query_descriptors.astype(np.float32)\n",
    "        distances, indices = index.search(query_descriptors, k)\n",
    "\n",
    "        # Map indices to product IDs\n",
    "        results = {}\n",
    "        for i in range(len(indices)):\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                if idx < len(product_ids):\n",
    "                    product_id = product_ids[idx]\n",
    "                    results[product_id] = results.get(product_id, 0) + 1\n",
    "\n",
    "        # Sort and return most frequent matches\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Top {k} matches: {sorted_results[:k]}\")\n",
    "    else:\n",
    "        print(\"No descriptors found for the query image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faiss index and metadata saved.\n",
      "Added 9 images to Faiss index.\n"
     ]
    }
   ],
   "source": [
    "new_images = [\n",
    "        (300001, \"https://m.media-amazon.com/images/I/31UISB90sYL._AC_UL320_.jpg\"),\n",
    "        (300002, \"https://m.media-amazon.com/images/I/51JFb7FctDL._AC_UL320_.jpg\"),\n",
    "        (300003, \"https://m.media-amazon.com/images/I/61ZzcguzB1L._AC_UL320_.jpg\"),\n",
    "        (300004, \"https://m.media-amazon.com/images/I/41O8pnwGG+L._AC_UL320_.jpg\"),\n",
    "        (300005, \"https://m.media-amazon.com/images/I/41e6oqY5-ZL._AC_UL320_.jpg\"),\n",
    "        (300006, \"https://m.media-amazon.com/images/I/81ZZPvIWnYL._AC_UL320_.jpg\"),\n",
    "        (300007, \"https://m.media-amazon.com/images/I/51PVY5idbhL._AC_UL320_.jpg\"),\n",
    "        (300008, \"https://m.media-amazon.com/images/I/51yB+3-eJwL._AC_UL320_.jpg\"),\n",
    "        (300009,\"https://m.media-amazon.com/images/I/71e1SSUhZeL._AC_UL320_.jpg\"),\n",
    "    ]\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "num_words = 100\n",
    "dimension = num_words\n",
    "add_descriptor_to_faiss(new_images)\n",
    "\n",
    "\n",
    "    # Search for similar images\n",
    "# search_similar_images(\"query_image.jpg\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 matches: [(300002, 92), (300006, 66), (300007, 41), (300008, 41), (300005, 31)]\n"
     ]
    }
   ],
   "source": [
    "search_similar_images(\"https://m.media-amazon.com/images/I/6161gqzzYlL._AC_UL320_.jpg\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/faiss_index.bin\", \"wb\") as f:\n",
    "    data = b\"Hello, world!\"  # Example binary data (string converted to bytes)\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4.]\n",
      " [2. 3. 4. 6.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.vstack(np.array([[1,2,3,4],[2,3,4,6]])).astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
